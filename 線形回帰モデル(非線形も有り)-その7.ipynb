{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1fe26909-f161-458d-b74c-d880726f844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "from scipy import integrate\n",
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "import math\n",
    "from random import Random\n",
    "\n",
    "%matplotlib inline\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a6c5e42a-21fd-4b6e-9844-f6f42fba298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_data_for_unit():\n",
    "    data_sets = pd.DataFrame()\n",
    "    beacon_files = os.listdir('raw_data/')\n",
    "    pos_files = os.listdir('POS_RAW_data/')\n",
    "\n",
    "    beacon_files.sort()\n",
    "    pos_files.sort()\n",
    "    \n",
    "    if beacon_files[0] == \".DS_Store\":\n",
    "        beacon_files.pop(0)\n",
    "\n",
    "    if pos_files[0] == \".DS_Store\":\n",
    "        pos_files.pop(0)\n",
    "\n",
    "    pos_files = list(map(lambda x: x[:14], pos_files))[::2]\n",
    "\n",
    "    for beacon_file, pos_file in zip(beacon_files, pos_files):\n",
    "        date_beacon = beacon_file[9:17]\n",
    "        date_pos    = pos_file[6:14]\n",
    "\n",
    "        if date_beacon != date_pos:\n",
    "            continue\n",
    "        else:\n",
    "            date = date_beacon\n",
    "        \n",
    "        t_start = datetime.strptime(f'{date} 09:00', '%Y%m%d %H:%M').timestamp()\n",
    "        t_end   = datetime.strptime(f'{date} 21:00', '%Y%m%d %H:%M').timestamp()\n",
    "        rssi    = 70\n",
    "        beacon_data = pd.read_csv(f\"raw_data/{beacon_file}\")\n",
    "        pos_data1 = pd.read_csv(f\"POS_RAW_data/{pos_file}_01.csv\")\n",
    "        pos_data2 = pd.read_csv(f\"POS_RAW_data/{pos_file}_02.csv\")\n",
    "\n",
    "        pos_data1 = pos_data1[[\"商品コード\", \"商品名称（または券名称）\", \"単価\", \"数量\", \"合計金額\"]]\n",
    "        pos_data2 = pos_data2[[\"商品コード\", \"商品名称（または券名称）\", \"単価\", \"数量\", \"合計金額\"]]\n",
    "        pos_data  = pd.concat([pos_data1, pos_data2])\n",
    "\n",
    "        pos_data[[\"単価\", \"数量\", \"合計金額\"]] = pos_data[[\"単価\", \"数量\", \"合計金額\"]].map(lambda x: int(x))\n",
    "        pos_data = pos_data.groupby([\"商品コード\", \"商品名称（または券名称）\"]).sum()\n",
    "\n",
    "        pos_data[\"単価\"] = pos_data[\"合計金額\"] / pos_data[\"数量\"]\n",
    "        pos_data[\"単価\"] = pos_data[\"単価\"].astype(int)\n",
    "\n",
    "        beacon_data.columns = [\"No.\", \"mac-address\", \"distance\", \"rssi\", \"random\", \"timestamp\"]\n",
    "        beacon_data = beacon_data[beacon_data[\"random\"] == 1]\n",
    "        beacon_data = beacon_data[beacon_data[\"timestamp\"] >= t_start]\n",
    "        beacon_data = beacon_data[beacon_data[\"timestamp\"] <= t_end]\n",
    "        beacon_data = beacon_data[beacon_data[\"rssi\"] < rssi]\n",
    "        beacon_data = beacon_data.drop_duplicates(\"mac-address\")\n",
    "\n",
    "        per_unit = pd.DataFrame(\n",
    "            {\n",
    "                \"date\":[date],\n",
    "                \"総ビーコン数\": [len(beacon_data)],\n",
    "                \"総売上点数\": [pos_data[\"数量\"].sum()],\n",
    "                \"総売上\": [pos_data[\"合計金額\"].sum()],\n",
    "            }\n",
    "        )\n",
    "        data_sets = pd.concat([data_sets, per_unit])\n",
    "\n",
    "    return data_sets.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "702fd7f9-2460-4e6e-8de4-ac4001fb58aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_distribution(lambda_, k):\n",
    "    return stats.poisson.pmf(round(k), lambda_)\n",
    "\n",
    "def normal_distribution(x, mu, sigma):\n",
    "    return 1 / np.sqrt(2 * np.pi * (sigma ** 2)) * np.exp(-((x - mu) ** 2) / (2 * (sigma ** 2)))\n",
    "\n",
    "def uniform_distribution(x, alpha, beta):\n",
    "    if alpha > beta:\n",
    "        alpha, beta = beta, alpha\n",
    "\n",
    "    if (alpha <= x) and (x <= beta):\n",
    "        return 1 / (beta - alpha)\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f948a5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Leap_Flog(h_dash, θ, p, ε, args=(), max_iterate=1e3):\n",
    "    ite = 0\n",
    "    while ite < max_iterate:\n",
    "        tmp = -1 / 2 * h_dash(θ, *args)\n",
    "        p = p + ε * tmp\n",
    "        θ = θ + ε * p\n",
    "\n",
    "        tmp = -1 / 2 * h_dash(θ, *args)\n",
    "        p = p + ε * tmp\n",
    "\n",
    "        ite = ite + 1\n",
    "    \n",
    "    return θ, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fb11325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Leap_Flog_Cus(h_dash, θ, p, ε, args=(), max_iterate=1e3):\n",
    "    ite = 0\n",
    "    α = 0.9\n",
    "    while ite < max_iterate:\n",
    "        tmp = -1 / 2 * h_dash(θ, *args)\n",
    "        p = (α * p + (1 - α) * ε * tmp) / α\n",
    "        θ = θ + ε * p\n",
    "\n",
    "        tmp = -1 / 2 * h_dash(θ, *args)\n",
    "        p = (α * p + (1 - α) * ε * tmp) / α\n",
    "\n",
    "        ite = ite + 1\n",
    "    \n",
    "    return θ, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e4653389-f059-4523-8815-65293ab69dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoissonRegression_On_Bayes:\n",
    "    def __init__(self, isGLMM=False, hamiltonian_ε=0.0001, hamiltonian_ite=100, gauss_quadrature_dim=3200, random_state=None):\n",
    "        self.alpha   = np.array([], dtype=np.float64)\n",
    "        self.alpha0  = np.float64(0.0)\n",
    "        self.isGLMM  = isGLMM\n",
    "        self.gauss_s = np.float64(0.1)\n",
    "        self.max_dim = gauss_quadrature_dim\n",
    "        self.hamiltonian_ε = hamiltonian_ε\n",
    "        self.hamiltonian_ite = hamiltonian_ite\n",
    "        self.sampling_alpha   = np.array([], dtype=np.float64)\n",
    "        self.sampling_alpha0  = np.array([], dtype=np.float64)\n",
    "        self.sampling_gauss_s = np.array([], dtype=np.float64)\n",
    "\n",
    "        if random_state != None:\n",
    "            self.random = np.random\n",
    "            self.random.seed(seed=random_state)\n",
    "        else:\n",
    "            self.random = np.random\n",
    "\n",
    "    def sampling(self, x_train, y_train, iter_num=1e6, burnin=1e3):\n",
    "        if (x_train.ndim != 2) or (y_train.ndim != 1):\n",
    "            print(f\"x_train dims = {x_train.ndim}\")\n",
    "            print(f\"y_train dims = {y_train.ndim}\")\n",
    "            print(\"エラー：：次元数が一致しません。\")\n",
    "            return False\n",
    "\n",
    "        if type(x_train) is pd.core.frame.DataFrame:\n",
    "            x_train = x_train.to_numpy()\n",
    "\n",
    "        if type(y_train) is pd.core.series.Series:\n",
    "            y_train = y_train.to_numpy()\n",
    "        \n",
    "        num, s = x_train.shape\n",
    "        y_train = y_train.reshape([num, 1])\n",
    "\n",
    "        #正規方程式\n",
    "        A = np.hstack([x_train, np.ones([num, 1])])\n",
    "        b = np.log(y_train).reshape([num])\n",
    "\n",
    "        x = np.dot(np.linalg.inv(np.dot(A.T, A)), np.dot(A.T, b))\n",
    "        self.alpha, self.alpha0 = x[0:s].reshape([1, s]), x[s]\n",
    "        self.gauss_s            = np.float64(0.1)\n",
    "\n",
    "        def likelihood_GLMM_diff(x, speci_dim):\n",
    "            alpha, alpha0, gauss_s = x[:-2], x[-2], x[-1]\n",
    "\n",
    "            exp_index   = (np.sum(alpha * x_train, axis=1) + alpha0).reshape([num, 1])\n",
    "            upper_lim   = exp_index < 650\n",
    "            upper_rev   = upper_lim == False\n",
    "            exp_index_T = exp_index * upper_lim\n",
    "            exp_index_F = 650       * upper_rev\n",
    "            \n",
    "            lambda_vec = np.exp(exp_index_T)\n",
    "            lambda_vec = lambda_vec.reshape([num, 1])\n",
    "\n",
    "            def vectorlize_quad(lambda_, y_train_indi, gauss_s, speci_dim):\n",
    "                def calc_likelihood(r, λ, k, s):\n",
    "                    λ_r  = λ * np.exp(r)\n",
    "                    poisson = poisson_distribution(λ_r, k)\n",
    "                    normal  = normal_distribution(r, 0, s)\n",
    "                    return poisson * normal\n",
    "\n",
    "                def calc_λ(r, λ, k, s):\n",
    "                    λ_r  = λ * np.exp(r)\n",
    "                    poisson = poisson_distribution(λ_r, k)\n",
    "                    normal  = normal_distribution(r, 0, s)\n",
    "                    return poisson * normal * λ\n",
    "\n",
    "                def calc_squared_r(r, λ, k, s):\n",
    "                    λ_r  = λ * np.exp(r)\n",
    "                    poisson = poisson_distribution(λ_r, k)\n",
    "                    normal  = normal_distribution(r, 0, s)\n",
    "                    return poisson * normal * (r ** 2)\n",
    "\n",
    "                range_σ    = max(10 * gauss_s, 1)\n",
    "                likelihood = integrate.fixed_quad(calc_likelihood, -range_σ, range_σ, args=(lambda_, y_train_indi, gauss_s), n=speci_dim)[0]\n",
    "                diff_alpha = integrate.fixed_quad(calc_λ,          -range_σ, range_σ, args=(lambda_, y_train_indi, gauss_s), n=speci_dim)[0]\n",
    "                diff_s     = integrate.fixed_quad(calc_squared_r,  -range_σ, range_σ, args=(lambda_, y_train_indi, gauss_s), n=speci_dim)[0]\n",
    "                \n",
    "                tmp = np.zeros(2)\n",
    "                tmp[0]   = diff_alpha / (likelihood + 1e-16)\n",
    "                tmp[1]   = diff_s     / (likelihood + 1e-16)\n",
    "                return tmp[0], tmp[1]\n",
    "\n",
    "            diff_calc         = np.frompyfunc(vectorlize_quad, 4, 2)(lambda_vec, y_train, gauss_s, speci_dim)\n",
    "            diff_alpha_calc   = diff_calc[0].astype(float).reshape([num, 1])\n",
    "            diff_gauss_s_calc = diff_calc[1].astype(float).reshape([num, 1])\n",
    "\n",
    "            diff_alpha_calc   = y_train - diff_alpha_calc\n",
    "            diff_gauss_s_calc = (diff_gauss_s_calc / (self.gauss_s ** 2) - 1) / self.gauss_s\n",
    "\n",
    "            diff_alpha_calc   = diff_alpha_calc   * upper_lim + (y_train - np.exp(exp_index_F).reshape([num, 1])) * upper_rev\n",
    "            diff_gauss_s_calc = diff_gauss_s_calc * upper_lim\n",
    "\n",
    "            diff_alpha   = np.sum(diff_alpha_calc * x_train, axis=0).reshape([1, s]) / num\n",
    "            diff_alpha0  = np.sum(diff_alpha_calc)                                   / num\n",
    "            diff_gauss_s = np.sum(diff_gauss_s_calc)                                 / num\n",
    "\n",
    "            return -np.hstack([diff_alpha.reshape(diff_alpha.shape[1]), diff_alpha0, diff_gauss_s])\n",
    "        \n",
    "        def vectorlize_quad(lambda_, y_train_indi, gauss_s, speci_dim):\n",
    "            def calc_likelihood(r, λ, k, s):\n",
    "                λ_r  = λ * np.exp(r)\n",
    "                poisson = poisson_distribution(λ_r, k)\n",
    "                normal  = normal_distribution(r, 0, s)\n",
    "                return poisson * normal\n",
    "\n",
    "            range_σ = max(10 * gauss_s, 1)\n",
    "            tmp = integrate.fixed_quad(calc_likelihood, -range_σ, range_σ, args=(lambda_, y_train_indi, gauss_s), n=speci_dim)[0]\n",
    "            tmp = np.log(tmp + 1e-16)\n",
    "            return tmp\n",
    "        \n",
    "        def likelihood_GLM_diff(x):\n",
    "            alpha, alpha0 = x[:-1], x[-1]\n",
    "            lambda_vec = np.exp(np.sum(alpha * x_train, axis=1) + alpha0)\n",
    "            lambda_vec = lambda_vec.reshape([num, 1])\n",
    "\n",
    "            diff_alpha_calc   = y_train - lambda_vec\n",
    "\n",
    "            diff_alpha   = np.sum(diff_alpha_calc * x_train, axis=0).reshape([1, s]) / num\n",
    "            diff_alpha0  = np.sum(diff_alpha_calc)                                   / num\n",
    "\n",
    "            return -np.hstack([diff_alpha.reshape(diff_alpha.shape[1]), diff_alpha0])\n",
    "        \n",
    "        def vectorlize_non_quad(lambda_, y_train_indi):\n",
    "            tmp = poisson_distribution(lambda_, y_train_indi)\n",
    "            tmp = np.log(tmp + 1e-16)\n",
    "            return tmp\n",
    "        \n",
    "        py_vectorlize_quad     = np.frompyfunc(vectorlize_quad, 4, 1)\n",
    "        py_vectorlize_non_quad = np.frompyfunc(vectorlize_non_quad, 2, 1)\n",
    "\n",
    "        now_ite  = 0\n",
    "        while now_ite < iter_num:\n",
    "            tmp_alpha, tmp_alpha0 = self.alpha, self.alpha0\n",
    "            tmp_gauss_s           = self.gauss_s\n",
    "\n",
    "            if self.isGLMM == True:\n",
    "                x = np.hstack([tmp_alpha.reshape(tmp_alpha.shape[1]), tmp_alpha0, tmp_gauss_s])\n",
    "                p = self.random.normal(loc=0, scale=1, size=x.shape)\n",
    "            else:\n",
    "                x = np.hstack([tmp_alpha.reshape(tmp_alpha.shape[1]), tmp_alpha0])\n",
    "                p = self.random.normal(loc=0, scale=1, size=x.shape)\n",
    "\n",
    "            lambda_vec = np.exp(np.sum(tmp_alpha * x_train, axis=1) + tmp_alpha0)\n",
    "            lambda_vec = lambda_vec.reshape([num, 1])\n",
    "            if self.isGLMM == True:\n",
    "                now_likelihood = py_vectorlize_quad(lambda_vec, y_train, tmp_gauss_s, self.max_dim).astype(float).reshape([num, 1])\n",
    "            else:\n",
    "                now_likelihood = py_vectorlize_non_quad(lambda_vec, y_train).astype(float).reshape([num, 1])\n",
    "            now_hamiltonian = -np.sum(now_likelihood) / num + np.sum(p ** 2) / 2\n",
    "            \n",
    "            if self.isGLMM == True:\n",
    "                hamiltonian_ε = np.array([self.hamiltonian_ε / np.mean(x_train), self.hamiltonian_ε, self.hamiltonian_ε])\n",
    "                x, p = Leap_Flog_Cus(likelihood_GLMM_diff, x, p, ε=hamiltonian_ε, args=(self.max_dim,), max_iterate=self.hamiltonian_ite)\n",
    "                tmp_alpha, tmp_alpha0, tmp_gauss_s = x[:-2].reshape(1, s), x[-2], x[-1]\n",
    "            else:\n",
    "                hamiltonian_ε = np.array([self.hamiltonian_ε / np.mean(x_train), self.hamiltonian_ε])\n",
    "                x, p = Leap_Flog_Cus(likelihood_GLM_diff,  x, p, ε=hamiltonian_ε,                       max_iterate=self.hamiltonian_ite)\n",
    "                tmp_alpha, tmp_alpha0              = x[:-1].reshape(1, s), x[-1]\n",
    "\n",
    "            lambda_vec = np.exp(np.sum(tmp_alpha * x_train, axis=1) + tmp_alpha0)\n",
    "            lambda_vec = lambda_vec.reshape([num, 1])\n",
    "            if self.isGLMM == True:\n",
    "                tmp_likelihood = py_vectorlize_quad(lambda_vec, y_train, tmp_gauss_s, self.max_dim).astype(float).reshape([num, 1])\n",
    "            else:\n",
    "                tmp_likelihood = py_vectorlize_non_quad(lambda_vec, y_train).astype(float).reshape([num, 1])\n",
    "            tmp_hamiltonian = -np.sum(tmp_likelihood) / num + np.sum(p ** 2) / 2\n",
    "\n",
    "            tmp_rand = self.random.random()\n",
    "            tmp_rate = np.exp(now_hamiltonian - tmp_hamiltonian)\n",
    "            if tmp_rand < tmp_rate:\n",
    "\n",
    "                if now_ite % 1 == 0:\n",
    "                    print(f\"現在ite:{now_ite+1}  保存サンプリング数:{len(self.sampling_alpha0)}\", flush=True)\n",
    "                \n",
    "                self.alpha, self.alpha0 = tmp_alpha, tmp_alpha0\n",
    "                self.gauss_s            = tmp_gauss_s\n",
    "\n",
    "                if now_ite > burnin:\n",
    "                    if now_ite == (burnin + 1):\n",
    "                        self.sampling_alpha   = np.array(self.alpha).reshape(  [1, s])\n",
    "                        self.sampling_alpha0  = np.array(self.alpha0).reshape( [1, 1])\n",
    "                        self.sampling_gauss_s = np.array(self.gauss_s).reshape([1, 1])\n",
    "                    else:\n",
    "                        self.sampling_alpha   = np.vstack((self.sampling_alpha,   self.alpha))\n",
    "                        self.sampling_alpha0  = np.vstack((self.sampling_alpha0,  self.alpha0))\n",
    "                        self.sampling_gauss_s = np.vstack((self.sampling_gauss_s, self.gauss_s))\n",
    "                now_ite  = now_ite  + 1\n",
    "\n",
    "        return True\n",
    "\n",
    "    def predict(self, x_test, sample=100, step=1):\n",
    "        self.alpha   = np.mean(self.sampling_alpha,   axis=0).reshape(self.alpha.shape)\n",
    "        self.alpha0  = np.mean(self.sampling_alpha0,  axis=0).reshape(self.alpha0.shape)\n",
    "        self.gauss_s = np.mean(self.sampling_gauss_s, axis=0).reshape(self.gauss_s.shape)\n",
    "\n",
    "        if self.isGLMM == True:\n",
    "            lambda_vec = np.exp(np.sum(self.alpha * x_test, axis=1) + self.alpha0)\n",
    "            range_σ    = max(10 * self.gauss_s, 1)\n",
    "            def vectorlize_quad(lambda_, y_test_indi):\n",
    "                def mixture(r):\n",
    "                    lambda_r  = lambda_ * np.exp(r)\n",
    "                    poisson   = poisson_distribution(lambda_r, y_test_indi)\n",
    "                    normal    = normal_distribution(r, 0, self.gauss_s)\n",
    "                    return poisson * normal\n",
    "                \n",
    "                tmp = integrate.fixed_quad(mixture, -range_σ, range_σ, self.max_dim)[0]\n",
    "                return y_test_indi, tmp\n",
    "\n",
    "            y_test_mat = (lambda_vec - sample/2 * step).reshape([len(lambda_vec), 1])\n",
    "            lambda_mat = (lambda_vec                  ).reshape([len(lambda_vec), 1])\n",
    "            for ite in np.arange(-sample / 2 * step + step, sample / 2 * step, step):\n",
    "                y_test_mat = np.hstack([y_test_mat, (lambda_vec + ite).reshape([len(lambda_vec), 1])])\n",
    "                lambda_mat = np.hstack([lambda_mat, (lambda_vec      ).reshape([len(lambda_vec), 1])])\n",
    "\n",
    "            pred_mat  = np.frompyfunc(vectorlize_quad, 2, 2)(lambda_mat, y_test_mat)\n",
    "            pred_y    = pred_mat[0].astype(float)\n",
    "            pred_prob = pred_mat[1].astype(float)\n",
    "            \n",
    "            return pred_y, pred_prob\n",
    "        else:\n",
    "            return np.exp(np.sum(self.alpha * x_test, axis=1) + self.alpha0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb13617b-1ccd-430b-95ff-0277a9431769",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets = agg_data_for_unit()\n",
    "\n",
    "X = data_sets.drop([\"date\", \"総売上点数\", \"総売上\"], axis=1)\n",
    "y = data_sets[\"総売上点数\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "model = PoissonRegression_On_Bayes(isGLMM=True)\n",
    "model.sampling(X_train, y_train, iter_num=10000, burnin=0)\n",
    "\n",
    "#data for 6400\n",
    "#iterate  20  time  5m  5s\n",
    "#iterate  30  time  7m 40s\n",
    "#iterate  40  time 11m 12s\n",
    "#iterate  50  time 13m 48s\n",
    "#iterate  60  time 16m  9s\n",
    "#iterate  70  time 18m 44s\n",
    "#iterate  80  time 21m  6s\n",
    "#iterate  90  time 23m 28s\n",
    "#iterate 100  time 25m 49s\n",
    "\n",
    "#predict\n",
    "#iterate   1000  time      4h 19m 10s\n",
    "#iterate  10000  time  1d 19h 11m 40s\n",
    "#iterate 100000  time 17d 23h 56m 40s\n",
    "\n",
    "#data for 3200\n",
    "#iterate  10  time  1m 26s\n",
    "#iterate  20  time  2m 43s\n",
    "#iterate  30  time  4m 02s\n",
    "#iterate  40  time  5m 28s\n",
    "#iterate  50  time  6m 46s\n",
    "#iterate  60  time  8m  3s\n",
    "#iterate  70  time  9m 21s\n",
    "#iterate  80  time 10m 39s\n",
    "#iterate  90  time 12m  4s\n",
    "#iterate 100  time 13m 21s\n",
    "\n",
    "#predict\n",
    "#iterate   1000  time      2h 12m 24s\n",
    "#iterate  10000  time     22h 04m 04s\n",
    "#iterate 100000  time  9d  4h 40m 44s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ab3ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 9))\n",
    "plt.subplots_adjust(wspace=0.15,hspace=0.4)\n",
    "\n",
    "plt.subplot(3,2,1)\n",
    "plt.plot(np.arange(0, model.sampling_alpha.shape[0], 1), model.sampling_alpha)\n",
    "plt.title(\"alphaのサンプリング過程\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(3,2,2)\n",
    "plt.hist(model.sampling_alpha[:, 0], bins=25)\n",
    "plt.title(\"p(α|D)の確率密度関数\")\n",
    "plt.xlabel(f\"N = {model.sampling_alpha.shape[0]}   σ = {0.01}\")\n",
    "plt.ylabel(\"頻度\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(3,2,3)\n",
    "plt.plot(np.arange(0, model.sampling_alpha0.shape[0], 1), model.sampling_alpha0)\n",
    "plt.title(\"alpha0のサンプリング過程\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(3,2,4)\n",
    "plt.hist(model.sampling_alpha0, bins=25)\n",
    "plt.title(\"p(α0|D)の確率密度関数\")\n",
    "plt.xlabel(f\"N = {model.sampling_alpha0.shape[0]}   σ = {0.01}\")\n",
    "plt.ylabel(\"頻度\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(3,2,5)\n",
    "plt.plot(np.arange(0, model.sampling_gauss_s.shape[0], 1), model.sampling_gauss_s)\n",
    "plt.title(\"gauss_sのサンプリング過程\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(3,2,6)\n",
    "plt.hist(model.sampling_gauss_s, bins=25)\n",
    "plt.title(\"p(gs|D)の確率密度関数\")\n",
    "plt.xlabel(f\"N = {model.sampling_gauss_s.shape[0]}   σ = {0.01}\")\n",
    "plt.ylabel(\"頻度\")\n",
    "plt.grid(True)\n",
    "\n",
    "print(\"Statistics information\")\n",
    "print(\"          mean      median      std    \")\n",
    "print(\"alpha     {:.5f}   {:.5f}     {:.5f}\".format(np.mean(model.sampling_alpha  ), np.median(model.sampling_alpha  ), np.std(model.sampling_alpha  )))\n",
    "print(\"alpha0    {:.5f}   {:.5f}     {:.5f}\".format(np.mean(model.sampling_alpha0 ), np.median(model.sampling_alpha0 ), np.std(model.sampling_alpha0 )))\n",
    "print(\"gauss_s   {:.5f}   {:.5f}     {:.5f}\".format(np.mean(model.sampling_gauss_s), np.median(model.sampling_gauss_s), np.std(model.sampling_gauss_s)))\n",
    "print(\"\")\n",
    "\n",
    "p_alpha   = np.percentile(model.sampling_alpha,   q=[0, 25, 50, 75, 100])\n",
    "p_alpha0  = np.percentile(model.sampling_alpha0,  q=[0, 25, 50, 75, 100])\n",
    "p_gauss_s = np.percentile(model.sampling_gauss_s, q=[0, 25, 50, 75, 100])\n",
    "print(\"quartiles\")\n",
    "print(\"          2.5%    25%      50%      75%      97.5%\")\n",
    "print(\"alpha     {:.5f} {:.5f}  {:.5f}  {:.5f}  {:.5f}\".format(p_alpha[0],   p_alpha[1],   p_alpha[2],   p_alpha[3],   p_alpha[4]))\n",
    "print(\"alpha0    {:.5f} {:.5f}  {:.5f}  {:.5f}  {:.5f}\".format(p_alpha0[0],  p_alpha0[1],  p_alpha0[2],  p_alpha0[3],  p_alpha0[4]))\n",
    "print(\"gauss_s   {:.5f} {:.5f}  {:.5f}  {:.5f}  {:.5f}\".format(p_gauss_s[0], p_gauss_s[1], p_gauss_s[2], p_gauss_s[3], p_gauss_s[4]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
